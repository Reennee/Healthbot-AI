{
  "model": "microsoft/DialoGPT-medium",
  "train_data": "backend/data/healthcare_conversations.json",
  "output_dir": "backend/models_pt_finetuned",
  "hyperparameters": {
    "num_train_epochs": 3,
    "per_device_train_batch_size": 4,
    "learning_rate": 5e-5,
    "weight_decay": 0.01,
    "max_length": 256,
    "fp16": true
  },
  "notes": "This configuration is recommended for a mid-scale fine-tune on a GPU-enabled machine. For CPU-only runs, reduce batch size and consider num_train_epochs=1 or 2. Use microsoft/DialoGPT-medium or -large only if you have GPU resources."
}